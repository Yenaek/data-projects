// Onboarding
Onboarding
| summarize  total_score = sum(Score)
| project total_score



//////////////////////////////////////////////////////////
// Challenge 1

.execute database script <|
// Create table for the books
.create-merge table Books(rf_id:string, book_title:string, publish_date:long, author:string, language:string, number_of_pages:long, weight_gram:long)
// Import data for books
// (Used data is utilzing catalogue from https://github.com/internetarchive/openlibrary )
.ingest into table Books ('https://kustodetectiveagency.blob.core.windows.net/digitown-books/books.csv.gz') with (ignoreFirstRecord=true)
// Create table for the shelves
.create-merge table Shelves (shelf:long, rf_ids:dynamic, total_weight:long) 
// Import data for shelves
.ingest into table Shelves ('https://kustodetectiveagency.blob.core.windows.net/digitown-books/shelves.csv.gz') with (ignoreFirstRecord=true)

// Check how much the missing book weighs - 1764
Books
| where (book_title == "De Revolutionibus Magnis Data")
| project rf_id, weight_gram

// Calculate the difference between the weight of each shelf and how much it currently weighs
// The shelf with largest difference will be the one the book belongs to
// According to the exercise there is a small margin of error between real weight and measurements so the difference is rarely zero
Shelves 
| mv-expand rf_id = rf_ids to typeof(string) 
| lookup Books on rf_id 
| summarize total_weight = avg(total_weight), current_weight = sum(weight_gram) by shelf = shelf 
| extend weight_difference = total_weight - current_weight
| order by weight_difference desc 
| project shelf, total_weight, current_weight, weight_difference


//////////////////////////////////////////////////////////
// Challenge 2

.execute database script <|
// Ingestion may take ~40sec to complete, total 5M+ records
.create-merge table Votes (Timestamp:datetime, vote:string, via_ip:string, voter_hash_id:string)
.ingest async into table Votes (@'https://kustodetectiveagency.blob.core.windows.net/digitown-votes/votes_1.csv.gz')
.ingest async into table Votes (@'https://kustodetectiveagency.blob.core.windows.net/digitown-votes/votes_2.csv.gz')
.ingest async into table Votes (@'https://kustodetectiveagency.blob.core.windows.net/digitown-votes/votes_3.csv.gz')

// Query that counts the votes:
Votes
| summarize Count=count() by vote
| as hint.materialized=true T
| extend Total = toscalar(T | summarize sum(Count))
| project vote, Percentage = round(Count*100.0 / Total, 1), Count
| order by Count

// let multipleVoters =
// Votes
// | summarize c = count() by voter_hash_id
// | where c > 1
// | project voter_hash_id;
Votes
// | where not(voter_hash_id in (multipleVoters))
// Lots of votes for same candidate from a single machine in less than a minute, so aggegate machine votes at the minute granularity
| extend dt = format_datetime(Timestamp, "yyyy-MM-dd HH:mm")
| summarize c = count() by via_ip, vote, dt
// And keep only the credible votes (filters out e.g. 200+ votes for one candidate in a single machine for a one minute period)
| where c < 50
// Sum votes because they are already aggegated by ip and candidate
| summarize Count = sum(c) by vote
// Rest is provided code by exercise
| as hint.materialized=true T
| extend Total = toscalar(T | summarize sum(Count))
| project vote, Percentage = round(Count*100.0 / Total, 1), Count
| order by Count



//////////////////////////////////////////////////////////
// Challenge 3

.execute database script <|
// Create the table with the traffic information.
// The data loading process estimated to take ~3-4min to complete (114M+ rows of data).
// Notes: VIN - is Vehicle ID 
.create-merge table Traffic (Timestamp:datetime, VIN:string, Ave:int, Street:int)
.ingest async into table Traffic (@'https://kustodetectiveagency.blob.core.windows.net/digitown-traffic/log_00000.csv.gz')
.ingest async into table Traffic (@'https://kustodetectiveagency.blob.core.windows.net/digitown-traffic/log_00001.csv.gz')
.ingest async into table Traffic (@'https://kustodetectiveagency.blob.core.windows.net/digitown-traffic/log_00002.csv.gz')


// Cars that passed by the bank
let passedByBank =
Traffic
| where Ave == 157 and Street == 148
| distinct VIN
;
// Cars moving during the robbery
let movingDuringRobbery =
Traffic
| where 
    Timestamp >= datetime('2022-10-16T08:17:00Z')
    and Timestamp < datetime('2022-10-16T08:32:00Z')
| distinct VIN
;
// Cars stopped during the robbery
let stoppedDuringRobbery =
Traffic
| distinct VIN
| where not(VIN in (movingDuringRobbery))
;
// Find all stopping places for 3 cars, for cars that passed by the bank and were stopped during the robbery
Traffic
| where VIN in (stoppedDuringRobbery) and VIN in (passedByBank)
| summarize arg_max(Timestamp, *) by VIN
| summarize c = count() by Ave, Street
| where c == 3