{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 Stack Overflow Developer Survey Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:48:42.512473Z",
     "start_time": "2020-02-19T14:48:42.503474Z"
    },
    "hidden": true
   },
   "source": [
    "This data analysis project uses the 2019 Stack Overflow Developer Survey Results dataset which can be found [here](https://insights.stackoverflow.com/survey). \n",
    "\n",
    "The survey had a total of 88,883 respondents, with 85 questions, ranging from general questions such as age, gender and academic background to professional questions such as what are the technologies worked with, employment status and more. The survey also included some questions regarding the respondent's interactions with Stack Overflow, but those were not a part of the objective of this analysis.\n",
    "\n",
    "The dataset has not been modified further than what is done in this Notebook. Originally, the dataset is already very clean and structured, which means most of the modifications were for the sake of making the data easier to visualize.\n",
    "\n",
    "The analysis uses the [pandas](https://pandas.pydata.org/) and [NumPy](https://numpy.org/) libraries to load and modify the data. For the visualization, I chose [Plotly](https://plot.ly/python/plotly-express/) (Express) for two reasons:\n",
    "\n",
    "* I am already familiar with [Matplotlib](https://matplotlib.org/) and so this is a chance to learn a new plotting library;\n",
    "* The interactivity provided by the charts in Plotly.\n",
    "\n",
    "Almost all the plots are created with Plotly Express, a high-level API in the Plotly library. With that said, the table plot used in the last part of the analysis is created through the more traditional Graph Objects API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The analysis has four main sections:\n",
    "\n",
    "1. [General Data](#General-Data): includes general data about the respondents, including the gender distribution, academic background, employment status and country;\n",
    "\n",
    "\n",
    "2. [Tech Stack](#Tech-Stack): all the technology-related data, including programming languages, database environments, operating systems, etc.;\n",
    "\n",
    "\n",
    "3. [Professional Life](#Professional-Life): data related to the professional lives of the respondents, such as work week hours, work location, most important job factors, etc.\n",
    "\n",
    "\n",
    "4. [Other](#Other): explores questions that combine data from the previous sections, like \"Programming Languages vs Annual Compensation\" or \"Work Location vs Work Week Hours\".\n",
    "\n",
    "The [Conclusions](#Conclusions) at the very end aggregate all the \"Key Takeaways\" extracted from each of the enumerated sections. It has nothing new, only a TL;DR with the key takeaways of this data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following are the guiding questions and/or objectives of this analysis. They are answered throughout the four outlined sections, in the \"Key Takeaways\" at the end of each section:\n",
    "\n",
    "1. What are the most used technologies (programming languages, database environments, operating systems, etc.);\n",
    "\n",
    "\n",
    "2. Where are respondents from and what are their work conditions and employment status;\n",
    "\n",
    "\n",
    "3. How does remote work factor in today's professional life (compensation and work week hours);\n",
    "\n",
    "\n",
    "4. Tech Stack data vs Compensation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.180503Z",
     "start_time": "2020-02-24T16:10:36.637077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.194515Z",
     "start_time": "2020-02-24T16:10:44.184510Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Color for the General section charts\n",
    "general_color = \"#60BD68\"\n",
    "# Gradient for the General section charts\n",
    "general_color_gradient = [\"#30ff00\", \"#29eb00\", \"#21d100\", \"#18b700\", \"#11a400\"]\n",
    "\n",
    "# Color for the Tech Stack section charts\n",
    "tech_stack_color = \"#5DA5DA\"\n",
    "# Gradient for the Tech Stack section charts\n",
    "tech_stack_color_gradient = [\"#009fff\", \"#008fe3\", \"#0084d1\", \"#0075b8\", \"#0069a4\"]\n",
    "\n",
    "# Color for the People section charts\n",
    "people_color = \"#DECF3F\"\n",
    "# Gradient for the People section charts\n",
    "people_color_gradient = [\"#f5ff00\", \"#dbe100\", \"#c1c200\", \"#b3b200\", \"#a4a000\"]\n",
    "\n",
    "# Font size for all charts\n",
    "chart_font_size = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These are functions created for the sake of facilitating common operations throughout the analysis, broken down into three categories for the sake of organization in the Notebook: general operations, data transformations and plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### General operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.202510Z",
     "start_time": "2020-02-24T16:10:44.197507Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def count_options_frequencies(replies: pd.Series) -> Counter:\n",
    "    \"\"\"\n",
    "    Given a Series of data entries of semicolon-separated options\n",
    "    (i.e., replies for multiple choice questions), count the frequencies\n",
    "    of each unique option. \n",
    "    The function also works for single choice questions as the split\n",
    "    doesn't affect the output.\n",
    "    \"\"\"\n",
    "    # Create a dict-like object to map the options to their frequencies\n",
    "    options_freqs = Counter()\n",
    "    \n",
    "    # Split each data value at semicolons and, with the resulting individual\\\n",
    "    # options, update their frequencies accordingly\n",
    "    [options_freqs.update(reply.split(\";\")) for reply in replies]\n",
    "    \n",
    "    return options_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.223512Z",
     "start_time": "2020-02-24T16:10:44.207513Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataframe(\n",
    "    options_counts: Counter, \n",
    "    headers: list,\n",
    "    ascend_sort: Optional[bool]=False\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a Counter with the frequencies of each reply option to a\n",
    "    single question, create a new dataframe based on that data.\n",
    "    \"\"\"\n",
    "    # Get the headers for each column\n",
    "    options_header = headers[0]\n",
    "    freqs_header = headers[1]\n",
    "    \n",
    "    # Get the data for each column\n",
    "    # The options are the keys from the Counter\n",
    "    options = list(options_counts)\n",
    "    # The frequencies are the values from the Counter\n",
    "    frequencies = [options_counts[option] for option in options_counts]\n",
    "\n",
    "    # Store the options and respective frequencies in a new DataFrame\n",
    "    new_df = pd\\\n",
    "        .DataFrame({\n",
    "            options_header: options,\n",
    "            freqs_header: frequencies\n",
    "        })\n",
    "\n",
    "    # Sort the dataframe in descending order of frequencies\n",
    "    new_df.sort_values(\n",
    "        by=freqs_header, \n",
    "        ascending=ascend_sort, \n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    # Reset the DataFrame index according to the sorted data\n",
    "    new_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Uniformize the options with some common string replacements\n",
    "    new_df[options_header] = new_df[options_header].replace(\n",
    "        {\"nan\": \"NA\", \"Other(s):\": \"Other(s)\"}, regex=False\n",
    "    )\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.235506Z",
     "start_time": "2020-02-24T16:10:44.227509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def unpivot_delimited_data(\n",
    "    series: pd.Series,\n",
    "    delimiter: str\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a Series where each data value is a group of delimiter-separated\n",
    "    options, unpivot the individual options into separate rows, keeping\n",
    "    the ids for each corresponding individual value.\n",
    "    \"\"\"\n",
    "    # The resulting Series has a multi-level index, where the first level\\\n",
    "    # represents a single reply and the second level represents the\\\n",
    "    # multiple options chosen for that reply\n",
    "    return series\\\n",
    "        .apply( lambda x: pd.Series(x.split(delimiter)) )\\\n",
    "        .stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.246514Z",
     "start_time": "2020-02-24T16:10:44.238506Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def merge_unpivoted_data(\n",
    "    unpivoted_df: pd.DataFrame,\n",
    "    other_df: pd.DataFrame,\n",
    "    other_df_key: str,\n",
    "    first_col_name: str,\n",
    "    second_col_name: str\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a DataFrame of unpivoted data and a DataFrame ready to be merged,\n",
    "    merge the latter to the former.\n",
    "    \"\"\"\n",
    "    # The current multi-level index is kept as columns before\\\n",
    "    # resetting the index. These columns are named level_0 and\\\n",
    "    # level_1, which means we now have a DataFrame with the old\\\n",
    "    # index and the unpivoted programming languages\n",
    "    unpivoted_df = unpivoted_df.reset_index()\n",
    "\n",
    "    # Merge the DF of unpivoted data with the other DF using the\\\n",
    "    # shared indices\n",
    "    # From the first DF, the key column is the one with the first\\\n",
    "    # level of indices. The key from the other DF is specified as\\\n",
    "    # an argument\n",
    "    merged_df = pd.merge(\n",
    "        unpivoted_df, \n",
    "        other_df, \n",
    "        left_on=\"level_0\", \n",
    "        right_on=other_df_key\n",
    "    )\n",
    "\n",
    "    # Keep only the columns of interest after the merge (during\\\n",
    "    # the merge, the first column ends up being called zero)\n",
    "    merged_df = merged_df[[0, second_col_name]]\n",
    "\n",
    "    # And rename the first column\n",
    "    merged_df.rename(\n",
    "        columns={0: first_col_name},\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.263509Z",
     "start_time": "2020-02-24T16:10:44.253509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sort_by_top(\n",
    "    df: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    top_n_list: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a DataFrame Series to the Categorical data type\n",
    "    using a list of values as basis. When sorting the column,\n",
    "    the order of the categories in the list is kept.\n",
    "    \"\"\"\n",
    "    # Convert the column with unpivoted data to Categorical,\\\n",
    "    # using the list of top n as the available categories\n",
    "    df[target_column] = pd.Categorical(\n",
    "        df[target_column], \n",
    "        top_n_list\n",
    "    )\n",
    "    \n",
    "    # When sorting this column of now Categorical values, the order\\\n",
    "    # of the options in the top n list will be kept\n",
    "    df.sort_values(target_column, ascending=False, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.275521Z",
     "start_time": "2020-02-24T16:10:44.268507Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_mode_df(\n",
    "    df: pd.DataFrame,\n",
    "    groupby_column: str,\n",
    "    mode_column: str\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a two-column DataFrame where the first column has the\n",
    "    unpivoted options to a single question and the second column\n",
    "    has replies of another question, find the mode of each of\n",
    "    those replies for each unique option in the first column.\n",
    "    These results are returned as a new two-column DataFrame.\n",
    "    \"\"\"\n",
    "    # Group the DataFrame by the first column and find the\\\n",
    "    # most common value in the second column for each option\\\n",
    "    # in the former\n",
    "    df_mode = df.groupby([groupby_column]).apply(\n",
    "        lambda single_option_df: \n",
    "            single_option_df[mode_column].mode() \n",
    "    )\n",
    "\n",
    "    # Reset the new DataFrame's index so that the data is\\\n",
    "    # properly put in distinct columns\n",
    "    df_mode = df_mode.reset_index()\n",
    "    \n",
    "    # Rename the columns after the headers were lost in the grouping\n",
    "    df_mode.rename(\n",
    "        columns={0: mode_column},\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return df_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.285511Z",
     "start_time": "2020-02-24T16:10:44.280515Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scatter_plot_preview(data_series: pd.Series) -> None:\n",
    "    \"\"\"\n",
    "    Plot a Series of raw data to find its limits visually.\n",
    "    \"\"\"\n",
    "    # Sort the data in ascending order\n",
    "    data_series = data_series.sort_values(ascending=True)\n",
    "    \n",
    "    # Plot the Series in a scatter plot to explore it visually\n",
    "    fig = px.scatter(\n",
    "        x=data_series, \n",
    "        y=range(0, data_series.shape[0])\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.303512Z",
     "start_time": "2020-02-24T16:10:44.289510Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_column_chart(\n",
    "    data: pd.DataFrame, \n",
    "    x_data: str, \n",
    "    y_data: str, \n",
    "    data_labels: str, \n",
    "    title: str, \n",
    "    color: str,\n",
    "    axes_titles: Optional[List[str]]=None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Given the necessary data, with the DataFrame columns for\n",
    "    the axes and data labels specified, plot a column chart.\n",
    "    \"\"\"\n",
    "    # Plot the chart\n",
    "    fig = px.bar(\n",
    "        data, x=x_data, y=y_data, \n",
    "        text=data_labels, \n",
    "        title=title\n",
    "    )\n",
    "\n",
    "    # Add the data labels inside the columns and change the color of the columns\n",
    "    fig.update_traces(\n",
    "        texttemplate=\"%{text:.2s}\", \n",
    "        textposition=\"inside\",\n",
    "        marker_color=color\n",
    "    )\n",
    "    \n",
    "    # If custom axes titles were passed, update the axes accordingly\n",
    "    if axes_titles:\n",
    "        fig.update_layout(\n",
    "            xaxis_title=axes_titles[0], \n",
    "            yaxis_title=axes_titles[1]\n",
    "        )\n",
    "    \n",
    "    # Adjust the font and center the title\n",
    "    fig.update_layout(\n",
    "        uniformtext_minsize=chart_font_size, \n",
    "        uniformtext_mode=\"hide\",\n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    # Show the finalized plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.320512Z",
     "start_time": "2020-02-24T16:10:44.307514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_bar_chart(\n",
    "    data: pd.DataFrame, \n",
    "    x_data: str, \n",
    "    y_data: str, \n",
    "    data_labels: str, \n",
    "    title: str, \n",
    "    color: str,\n",
    "    axes_titles: Optional[List[str]]=None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Given the necessary data, with the DataFrame columns for\n",
    "    the axes and data labels specified, plot a bar chart.\n",
    "    \"\"\"\n",
    "    # Plot the chart\n",
    "    fig = px.bar(\n",
    "        data, y=y_data, x=x_data, \n",
    "        text=data_labels, \n",
    "        title=title, \n",
    "        orientation=\"h\"\n",
    "    )\n",
    "    \n",
    "    # Add the data labels inside the columns and change the color of the columns\n",
    "    fig.update_traces(\n",
    "        texttemplate=\"%{text:.2s}\", \n",
    "        textposition=\"inside\", \n",
    "        marker_color=color\n",
    "    )\n",
    "    \n",
    "    # If custom axes titles were passed, update the axes accordingly\n",
    "    if axes_titles:\n",
    "        fig.update_layout(\n",
    "            xaxis_title=axes_titles[0], \n",
    "            yaxis_title=axes_titles[1]\n",
    "        )\n",
    "    \n",
    "    # Adjust the font, use a log scale for the X axis and center the title \n",
    "    fig.update_layout(\n",
    "        uniformtext_minsize=chart_font_size, \n",
    "        uniformtext_mode=\"hide\", \n",
    "        xaxis_type=\"log\", \n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    # Show the finalized plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.332509Z",
     "start_time": "2020-02-24T16:10:44.323511Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_pie_chart(\n",
    "    data: pd.DataFrame, \n",
    "    values: str, \n",
    "    names: str, \n",
    "    title: str, \n",
    "    color: List[str],\n",
    "    donut_hole: Optional[float]=0.0, \n",
    "    show_legend: Optional[bool]=True\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot a pie chart, which can be a donut chart with\n",
    "    an optional legend.\n",
    "    \"\"\"\n",
    "    # Plot the chart\n",
    "    fig = px.pie(\n",
    "        data, values=values, names=names,\n",
    "        title=title, \n",
    "        hole=donut_hole, \n",
    "        color_discrete_sequence=color\n",
    "    )\n",
    "    \n",
    "    # Show the data labels and the percentages, outside the wedges\n",
    "    fig.update_traces(\n",
    "        textinfo=\"label+percent\", \n",
    "        textposition=\"outside\"\n",
    "    )\n",
    "    \n",
    "    # Adjust the font, hide/show the legend and center the title\n",
    "    fig.update_layout(\n",
    "        uniformtext_minsize=chart_font_size, \n",
    "        uniformtext_mode=\"hide\", \n",
    "        showlegend=show_legend, \n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    # Show the finalized plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.342511Z",
     "start_time": "2020-02-24T16:10:44.335509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_histogram(\n",
    "    x_series: pd.Series,\n",
    "    n_bins: int,\n",
    "    title: str,\n",
    "    color: str,\n",
    "    axes_titles: Optional[List[str]]=None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot an histogram of an already binned Series of\n",
    "    data using count as the aggregate function.\n",
    "    \"\"\"\n",
    "    # Plot the chart\n",
    "    fig = px.histogram(\n",
    "        x=x_series,\n",
    "        nbins=n_bins,\n",
    "        title=title\n",
    "    )\n",
    "    \n",
    "    # Change the color of the columns\n",
    "    fig.update_traces(\n",
    "        marker_color=color\n",
    "    )\n",
    "    \n",
    "    # If custom axes titles were passed, update the axes accordingly\n",
    "    if axes_titles:\n",
    "        fig.update_layout(\n",
    "            xaxis_title=axes_titles[0], \n",
    "            yaxis_title=axes_titles[1]\n",
    "        )\n",
    "        \n",
    "    # Adjust the font, remove horizontal gaps between the bins and center the title\n",
    "    fig.update_layout(\n",
    "        uniformtext_minsize=chart_font_size, \n",
    "        uniformtext_mode=\"hide\", \n",
    "        bargap=0, \n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    # Show the finalized plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.358511Z",
     "start_time": "2020-02-24T16:10:44.346509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_mult_histogram(\n",
    "    df: pd.DataFrame,\n",
    "    x_column: str,\n",
    "    categories_col: str,\n",
    "    n_bins: int,\n",
    "    title: str,\n",
    "    axes_titles: Optional[List[str]]=None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot multiple histograms for the same columns (break down\n",
    "    each bin by category).\n",
    "    \"\"\"\n",
    "    # Given a single dataframe, plot an histogram, specifying one\\\n",
    "    # column for the X axis and another to represent the categories\\\n",
    "    # of each bin\n",
    "    fig = px.histogram(\n",
    "        df, \n",
    "        x=x_column, \n",
    "        color=categories_col,\n",
    "        nbins=n_bins,\n",
    "        title=title\n",
    "    )\n",
    "    \n",
    "    # If custom axes titles were passed, update the axes accordingly\n",
    "    if axes_titles:\n",
    "        fig.update_layout(\n",
    "            xaxis_title=axes_titles[0], \n",
    "            yaxis_title=axes_titles[1]\n",
    "        )\n",
    "        \n",
    "    # Adjust the font, remove horizontal gaps between the bins and center the title\n",
    "    fig.update_layout(\n",
    "        uniformtext_minsize=chart_font_size, \n",
    "        uniformtext_mode=\"hide\", \n",
    "        bargap=0, \n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    # Show the finalized plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:44.370508Z",
     "start_time": "2020-02-24T16:10:44.361507Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_table(\n",
    "    df: pd.DataFrame,\n",
    "    column_headers: List[str],\n",
    "    data_columns: List[str],\n",
    "    title: str\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Given a DataFrame, plot a two-column table using its data.\n",
    "    \"\"\"\n",
    "    # Plot the table using the custom column headers received\\\n",
    "    # and the Series of the DataFrame as the data to be displayed\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=column_headers,\n",
    "                    fill_color=\"paleturquoise\", \n",
    "                    align=\"left\"\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        df[ data_columns[0] ], \n",
    "                        df[ data_columns[1] ]\n",
    "                    ],\n",
    "                    fill_color=\"lavender\",\n",
    "                    align=\"left\"\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    # Add an horizontally-centered text\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        title_x=0.5\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:47.585892Z",
     "start_time": "2020-02-24T16:10:44.372507Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "survey_results = pd.read_csv(\"../data/survey_results_public.csv\")\n",
    "\n",
    "# Number of respondents\n",
    "num_respondents = survey_results.shape[0]\n",
    "\n",
    "# Fill missing numerical series' data with zeroes for the time being\n",
    "survey_results[\"Age\"] = survey_results[\"Age\"].fillna(0)\n",
    "survey_results[\"WorkWeekHrs\"] = survey_results[\"WorkWeekHrs\"].fillna(0)\n",
    "survey_results[\"ConvertedComp\"] = survey_results[\"ConvertedComp\"].fillna(0)\n",
    "\n",
    "# General data\n",
    "ages = survey_results[\"Age\"]\n",
    "genders = survey_results[\"Gender\"].astype(\"str\")\n",
    "ed_levels = survey_results[\"EdLevel\"].astype(\"str\")\n",
    "employment_statuses = survey_results[\"Employment\"].astype(\"str\")\n",
    "countries = survey_results[\"Country\"].astype(\"str\")\n",
    "\n",
    "# Tech Stack data\n",
    "languages = survey_results[\"LanguageWorkedWith\"].astype(\"str\")\n",
    "platforms = survey_results[\"PlatformWorkedWith\"].astype(\"str\")\n",
    "databases = survey_results[\"DatabaseWorkedWith\"].astype(\"str\")\n",
    "web_frameworks = survey_results[\"WebFrameWorkedWith\"].astype(\"str\")\n",
    "dev_envs = survey_results[\"DevEnviron\"].astype(\"str\")\n",
    "op_sys = survey_results[\"OpSys\"].astype(\"str\")\n",
    "\n",
    "# Professional Life data\n",
    "underg_majors = survey_results[\"UndergradMajor\"].astype(\"str\")\n",
    "prog_usages = survey_results[\"MainBranch\"].astype(\"str\")\n",
    "work_week_hrs = survey_results[\"WorkWeekHrs\"]\n",
    "work_locs = survey_results[\"WorkLoc\"].astype(\"str\")\n",
    "job_factors = survey_results[\"JobFactors\"].astype(\"str\")\n",
    "resume_updates = survey_results[\"ResumeUpdate\"].astype(\"str\")\n",
    "annual_compensations = survey_results[\"ConvertedComp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### General Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This first section explores general data about the respondents, including their age, gender, country, academic background, and more.\n",
    "As such, this section is useful to get to know more about the people that completed the survey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:52.634980Z",
     "start_time": "2020-02-24T16:10:47.589376Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Explore the raw data visually to find its limits\n",
    "# scatter_plot_preview(ages) # 9-90 years\n",
    "\n",
    "# Mark the outliers as missing data (younger than\\\n",
    "# 9 years and older than 90)\n",
    "ages = ages.where(ages >= 9, np.NaN)\n",
    "ages = ages.where(ages <= 90, np.NaN)\n",
    "\n",
    "# Create the bin labels\n",
    "ages_labels = pd.Series([f\"[{i}, {i+10})\" for i in range(0, 81, 10)])\n",
    "\n",
    "# Create the bin intervals (closed on the left side)\n",
    "ages_bins = pd.IntervalIndex.from_tuples(\n",
    "    [(i, i+10) for i in range(0, 81, 10)],\n",
    "    closed=\"left\"\n",
    ")\n",
    "\n",
    "# Bin the respondent ages into nine bins\n",
    "ages = pd.cut(\n",
    "    ages, ages_bins, \n",
    "    right=False, \n",
    "    labels=ages_labels, \n",
    "    precision=0, \n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Drop missing values\n",
    "ages = ages.dropna()\n",
    "\n",
    "# Sort the binned data in ascending order\n",
    "ages.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# Change the values from categorical to string to be able to plot them\n",
    "ages = ages.astype(\"str\")\n",
    "\n",
    "# Plot an histogram for the binned ages\n",
    "plot_histogram(\n",
    "    ages,\n",
    "    ages_bins.shape[0], \n",
    "    \"Age Distribution of Respondents\", \n",
    "    general_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:53.328319Z",
     "start_time": "2020-02-24T16:10:52.638957Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Group the options stored for gender using different labels\n",
    "genders = genders.map(\n",
    "    lambda response: \n",
    "        \"NA\" if (response == \"nan\")\n",
    "        else \"Man\" if (response == \"Man\")\n",
    "        else \"Woman\" if (response == \"Woman\")\n",
    "        else \"Other\"\n",
    ")\n",
    "\n",
    "# Count the frequencies of each gender option\n",
    "genders_counts = count_options_frequencies(genders)\n",
    "\n",
    "# Create a DataFrame for the genders and their frequencies\n",
    "genders_df = prepare_dataframe(\n",
    "    genders_counts, \n",
    "    [\"Gender\", \"Respondents\"]\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "genders_plot = genders_df[genders_df[\"Gender\"] != \"NA\"]\n",
    "\n",
    "# Plot a pie chart with the gender distribution\n",
    "plot_pie_chart(\n",
    "    genders_plot, \n",
    "    \"Respondents\", \n",
    "    \"Gender\", \n",
    "    \"Gender Distribution of Respondents\",\n",
    "    general_color_gradient,\n",
    "    show_legend=False,\n",
    "    donut_hole=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Education Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:53.848853Z",
     "start_time": "2020-02-24T16:10:53.331301Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reword the options stored for education level\n",
    "ed_levels = ed_levels.map(\n",
    "    lambda response:\n",
    "        \"NA\" if (response == \"nan\")\n",
    "        else \"No formal education\" if (response == \"I never completed any formal education\")\n",
    "        else \"Primary School\" if (response == \"Primary/elementary school\")\n",
    "        else \"Secondary School\" if (response == \"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\")\n",
    "        else \"Higher ed. study w/o Degree\" if (response == \"Some college/university study without earning a degree\")\n",
    "        else \"Associate Degree\" if (response == \"Associate degree\")\n",
    "        else \"Bachelor’s Degree\" if (response == \"Bachelor’s degree (BA, BS, B.Eng., etc.)\")\n",
    "        else \"Master’s Degree\" if (response == \"Master’s degree (MA, MS, M.Eng., MBA, etc.)\")\n",
    "        else \"Professional Degree\" if (response == \"Professional degree (JD, MD, etc.)\")\n",
    "        else \"Doctoral Degree\"\n",
    ")\n",
    "\n",
    "# Count the frequencies of each education level option\n",
    "ed_levels_counts = count_options_frequencies(ed_levels)\n",
    "\n",
    "# Create a DataFrame for the education level options and\\\n",
    "# their frequencies\n",
    "ed_levels_df = prepare_dataframe(\n",
    "    ed_levels_counts, \n",
    "    [\"EducationLevel\", \"Respondents\"], \n",
    "    ascend_sort=True\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "ed_levels_plot = ed_levels_df[ed_levels_df[\"EducationLevel\"] != \"NA\"]\n",
    "\n",
    "# Plot a bar chart with education level options frequencies\n",
    "plot_bar_chart(\n",
    "    ed_levels_plot,\n",
    "    \"Respondents\", \"EducationLevel\",\n",
    "    \"Respondents\",\n",
    "    \"Education Level of Respondents\",\n",
    "    general_color,\n",
    "    axes_titles=[\"Respondents\", None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Undergrad Major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:54.704212Z",
     "start_time": "2020-02-24T16:10:53.851855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reword the options stored for undergrad majors\n",
    "underg_majors = underg_majors.map(\n",
    "    lambda response:\n",
    "        \"NA\" if (response == \"nan\")\n",
    "        else \"CompSci/SoftEng\" if (response == \"Computer science, computer engineering, or software engineering\")\n",
    "        else \"Web Development/Design\" if (response == \"Web development or web design\")\n",
    "        else \"InfoTech/SysAdmin\" if (response == \"Information systems, information technology, or system administration\")\n",
    "        else \"Mathematics/Statistics\" if (response == \"Mathematics or statistics\")\n",
    "        else \"Another Eng. Discipline\" if (response == \"Another engineering discipline (ex. civil, electrical, mechanical)\")\n",
    "        else \"Business\" if (response == \"A business discipline (ex. accounting, finance, marketing)\")\n",
    "        else \"Health Science\" if (response == \"A health science (ex. nursing, pharmacy, radiology)\")\n",
    "        else \"Humanities\" if (response == \"A humanities discipline (ex. literature, history, philosophy)\")\n",
    "        else \"Natural Science\" if (response == \"A natural science (ex. biology, chemistry, physics)\")\n",
    "        else \"Social Science\" if (response == \"A social science (ex. anthropology, psychology, political science)\")\n",
    "        else \"Fine Arts/Performing Arts\" if (response == \"Fine arts or performing arts (ex. graphic design, music, studio art)\")\n",
    "        else \"None\" if (response == \"I never declared a major\")\n",
    "        else \"Other\"\n",
    ")\n",
    "\n",
    "# Count the frequencies of each undegrad major option\n",
    "underg_majors_counts = count_options_frequencies(underg_majors)\n",
    "\n",
    "# Create a DataFrame for the undergrad majors and their frequencies\n",
    "underg_majors_df = prepare_dataframe(\n",
    "    underg_majors_counts, \n",
    "    [\"UndergradMajor\", \"Respondents\"],\n",
    "    ascend_sort=True\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "underg_majors_plot = underg_majors_df[\n",
    "    underg_majors_df[\"UndergradMajor\"] != \"NA\"\n",
    "]\n",
    "\n",
    "# Plot a bar chart for the undergrad majors\n",
    "plot_bar_chart(\n",
    "    underg_majors_plot,\n",
    "    \"Respondents\", \"UndergradMajor\",\n",
    "    \"Respondents\",\n",
    "    \"Most Common Undergrad Majors\",\n",
    "    general_color,\n",
    "    axes_titles=[\"Respondents\", None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Employment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:55.519769Z",
     "start_time": "2020-02-24T16:10:54.707198Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reword the options stored for employment status\n",
    "employment_statuses = employment_statuses.map(\n",
    "    lambda response:\n",
    "        \"NA\" if (response == \"nan\")\n",
    "        else \"Not looking for work\" if (response == \"Not employed, and not looking for work\")\n",
    "        else \"Looking for work\" if (response == \"Not employed, but looking for work\")\n",
    "        else \"Full-time\" if (response == \"Employed full-time\")\n",
    "        else \"Part-time\" if (response == \"Employed part-time\")\n",
    "        else \"Freelancer/Self-employed\" if (response == \"Independent contractor, freelancer, or self-employed\")\n",
    "        else \"Retired\" if (response == \"Retired\")\n",
    "        else \"Other\"\n",
    ")\n",
    "\n",
    "# Count the frequencies of each employment status option\n",
    "employment_statuses_counts = count_options_frequencies(employment_statuses)\n",
    "\n",
    "# Create a DataFrame for the employment statuses and their frequencies\n",
    "employment_statuses_df = prepare_dataframe(\n",
    "    employment_statuses_counts, \n",
    "    [\"EmploymentStatus\", \"Respondents\"], \n",
    "    ascend_sort=True\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "employment_statuses_plot = employment_statuses_df[\n",
    "    employment_statuses_df[\"EmploymentStatus\"] != \"NA\"\n",
    "]\n",
    "\n",
    "# Plot a bar chart with the employment statuses\n",
    "plot_bar_chart(\n",
    "    employment_statuses_plot,\n",
    "    \"Respondents\", \"EmploymentStatus\",\n",
    "    \"Respondents\",\n",
    "    f\"Employment Status of Respondents\",\n",
    "    general_color,\n",
    "    axes_titles=[\"Respondents\", None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:56.214996Z",
     "start_time": "2020-02-24T16:10:55.529774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Count the frequencies of each country\n",
    "countries_counts = count_options_frequencies(countries)\n",
    "\n",
    "# Create a DataFrame for the countries and their frequencies\n",
    "countries_df = prepare_dataframe(\n",
    "    countries_counts, \n",
    "    [\"Country\", \"Respondents\"], \n",
    "    ascend_sort=True\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "countries_plot = countries_df[countries_df[\"Country\"] != \"NA\"]\n",
    "\n",
    "# Plot a bar chart with the top 10 countries\n",
    "plot_bar_chart(\n",
    "    countries_plot.tail(n=10),\n",
    "    \"Respondents\", \"Country\",\n",
    "    \"Respondents\",\n",
    "    f\"Top 10 Countries with most Respondents\",\n",
    "    general_color,\n",
    "    axes_titles=[\"Respondents\", None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Over 40% of the respondents were between the ages of 20 and 30 and almost 70% were between 20 and 40 years-old;\n",
    "\n",
    "\n",
    "* Over 90% of the inquired were male, with less than 8% being of the female gender. The remaining 2% identified as another gender;\n",
    "\n",
    "\n",
    "* About 43% of the respondents completed a Bachelor's Degree, but only half of those have gone on to complete a Master's Degree;\n",
    "\n",
    "\n",
    "* More than half of respondents come from a Computer Science or Software Engineering academic background. The remaining inquired come primarily from other Engineering disciplines, Information Technologies, System Administration or Web Design and/or Development;\n",
    "\n",
    "\n",
    "* 70% of the respondents have a full-time job, with freelance and/or self-employment being the second most common employment status;\n",
    "\n",
    "\n",
    "* The five countries with most respondents are, from most to least, the United States of America, India, Germany, the United Kingdom and Canada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tech Stack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The \"Tech Stack\" data comprehends questions regarding what technologies and technical skills the respondents are proficient with. \n",
    "\n",
    "In other words, the Tech Stack section is about answering questions such as \"Which are the most used programming languages?\", \"Which frameworks are used for web development?\" or even \"Which are the respondents' favorite development environments/IDEs?\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Programming Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:57.158543Z",
     "start_time": "2020-02-24T16:10:56.217998Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract the individual languages from each\\\n",
    "# reply to count their frequencies\n",
    "languages_counts = count_options_frequencies(languages)\n",
    "\n",
    "# Create a DataFrame for the languages and their frequencies\n",
    "languages_df = prepare_dataframe(\n",
    "    languages_counts, \n",
    "    [\"Language\", \"Respondents\"]\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "languages_plot = languages_df[languages_df[\"Language\"] != \"NA\"]\n",
    "\n",
    "# Plot a column chart for the top 10 languages\n",
    "plot_column_chart(\n",
    "    languages_plot.head(n=10),\n",
    "    \"Language\", \"Respondents\",\n",
    "    \"Respondents\",\n",
    "    \"Top 10 Programming Languages\",\n",
    "    tech_stack_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Database Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:57.864761Z",
     "start_time": "2020-02-24T16:10:57.163544Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract the individual databases from each\\\n",
    "# reply to count their frequencies\n",
    "databases_counts = count_options_frequencies(databases)\n",
    "\n",
    "# Create a DataFrame for the databases and their frequencies\n",
    "databases_df = prepare_dataframe(\n",
    "    databases_counts, \n",
    "    [\"Database\", \"Respondents\"]\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "databases_plot = databases_df[databases_df[\"Database\"] != \"NA\"]\n",
    "\n",
    "# Plot a column chart for the top 10 databases\n",
    "plot_column_chart(\n",
    "    databases_plot.head(n=10),\n",
    "    \"Database\", \"Respondents\",\n",
    "    \"Respondents\",\n",
    "    \"Top 10 Database Environments\",\n",
    "    tech_stack_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Web Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:58.923787Z",
     "start_time": "2020-02-24T16:10:57.866741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract the individual web frameworks from each\\\n",
    "# reply to count their frequencies\n",
    "web_frameworks_counts = count_options_frequencies(web_frameworks)\n",
    "\n",
    "# Create a DataFrame for the web frameworks and their frequencies\n",
    "web_frameworks_df = prepare_dataframe(\n",
    "    web_frameworks_counts, \n",
    "    [\"WebFramework\", \"Respondents\"]\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "web_frameworks_plot = web_frameworks_df[\n",
    "    web_frameworks_df[\"WebFramework\"] != \"NA\"\n",
    "]\n",
    "\n",
    "# Plot a column chart for the top 10 web frameworks\n",
    "plot_column_chart(\n",
    "    web_frameworks_plot.head(n=10),\n",
    "    \"WebFramework\", \"Respondents\",\n",
    "    \"Respondents\",\n",
    "    \"Top 10 Web Frameworks\",\n",
    "    tech_stack_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Development Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:10:59.500679Z",
     "start_time": "2020-02-24T16:10:58.927328Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract the individual development environments from each\\\n",
    "# reply to count their frequencies\n",
    "dev_envs_counts = count_options_frequencies(dev_envs)\n",
    "\n",
    "# Create a DataFrame for the development environments and\\\n",
    "# their frequencies\n",
    "dev_envs_df = prepare_dataframe(\n",
    "    dev_envs_counts, \n",
    "    [\"DevEnv\", \"Respondents\"]\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "dev_envs_plot = dev_envs_df[dev_envs_df[\"DevEnv\"] != \"NA\"]\n",
    "\n",
    "# Plot a column chart for the top 5 development environments\n",
    "plot_column_chart(\n",
    "    dev_envs_plot.head(n=5),\n",
    "    \"DevEnv\", \"Respondents\",\n",
    "    \"Respondents\",\n",
    "    \"Top 5 Favorite IDEs/Development Environments\",\n",
    "    tech_stack_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Operating Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:00.200195Z",
     "start_time": "2020-02-24T16:10:59.504676Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Count the frequencies of each operating system\n",
    "op_sys_counts = count_options_frequencies(op_sys)\n",
    "\n",
    "# Create a dataframe for the frequencies\n",
    "op_sys_df = prepare_dataframe(\n",
    "    op_sys_counts, \n",
    "    [\"OperatingSystem\", \"Respondents\"]\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "op_sys_plot = op_sys_df[op_sys_df[\"OperatingSystem\"] != \"NA\"]\n",
    "\n",
    "# Plot a pie chart for the operating system distribution\n",
    "plot_pie_chart(\n",
    "    op_sys_plot, \n",
    "    \"Respondents\", \n",
    "    \"OperatingSystem\", \n",
    "    f\"Operating System Usage\",\n",
    "    tech_stack_color_gradient,\n",
    "    show_legend=False,\n",
    "    donut_hole=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Over 60% of the respondents use JavaScript and/or HTML/CSS, over 50% use SQL and 40% use Python and/or Java;\n",
    "\n",
    "\n",
    "* Over 46% of the respondents use MySQL for database environments. On the other hand, only about 23% use PostgreSQL and/or Microsoft SQL Server, the second and third most popular environments, respectively;\n",
    "\n",
    "\n",
    "* jQuery is still the most used Web Framework, used by almost a third of the inquired. React.js comes in second place (22%) and Angular(.js) in a very close third place;\n",
    "\n",
    "\n",
    "* In a group of 10 developers, about 5 of them reported using Visual Studio Code as one of their IDEs/Development Environments. About 3 people in the group use Visual Studio and/or Notepad++ as well;\n",
    "\n",
    "\n",
    "* Windows is the operating system of choice of almost half of the respondents. The remaining half is fairly evenly divided between MacOS and Linux-based systems, but the former takes second place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Professional Life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This third section pertains to getting to know the professional lives of the respondents: their work location (remote vs office), what are the most important job factors for them, what role programming plays in their professional lives, annual compensation and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Professional Usage of Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:00.964960Z",
     "start_time": "2020-02-24T16:11:00.203176Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reword the options stored for programming usages\n",
    "prog_usages = prog_usages.map(\n",
    "    lambda response:\n",
    "        \"NA\" if (response == \"nan\")\n",
    "        else \"Student\" if (response == \"I am a student who is learning to code\")\n",
    "        else \"Code as part of job\" if (response == \"I am not primarily a developer, but I write code sometimes as part of my work\")\n",
    "        else \"Hobbyist\" if (response == \"I code primarily as a hobby\")\n",
    "        else \"Professional developer\" if (response == \"I am a developer by profession\")\n",
    "        else \"Used to work as developer\" if (response == \"I used to be a developer by profession, but no longer am\")\n",
    "        else \"Other\"\n",
    ")\n",
    "\n",
    "# Count the frequencies of each programming usage option\n",
    "prog_usages_counts = count_options_frequencies(prog_usages)\n",
    "\n",
    "# Create a DataFrame for the programming usages and their frequencies\n",
    "prog_usages_df = prepare_dataframe(\n",
    "    prog_usages_counts, \n",
    "    [\"ProgUsages\", \"Respondents\"]\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "prog_usages_plot = prog_usages_df[prog_usages_df[\"ProgUsages\"] != \"NA\"]\n",
    "\n",
    "# Plot a column chart for the programming usages\n",
    "plot_column_chart(\n",
    "    prog_usages_plot,\n",
    "    \"ProgUsages\", \"Respondents\",\n",
    "    \"Respondents\",\n",
    "    \"Professional Usage of Programming\",\n",
    "    people_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Work Week Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:03.878682Z",
     "start_time": "2020-02-24T16:11:00.967936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Explore the raw data visually to find its limits\n",
    "# scatter_plot_preview(work_week_hrs) # 8-90 hours\n",
    "\n",
    "# Mark the outliers as missing data (less than 8 hours\\\n",
    "# and more than 90)\n",
    "work_week_hrs = work_week_hrs.where(work_week_hrs >= 8, np.NaN)\n",
    "work_week_hrs = work_week_hrs.where(work_week_hrs <= 90, np.NaN)\n",
    "\n",
    "# Create the bin labels\n",
    "work_week_hrs_labels = pd.Series([f\"[{i}, {i+10})\" for i in range(0, 81, 10)])\n",
    "\n",
    "# Create the bin intervals (closed on the left)\n",
    "work_week_hrs_bins = pd.IntervalIndex.from_tuples(\n",
    "    [(i, i+10) for i in range(0, 81, 10)], \n",
    "    closed=\"left\"\n",
    ")\n",
    "\n",
    "# Bin the work week hours Series into nine bins\n",
    "work_week_hrs = pd.cut(\n",
    "    work_week_hrs, work_week_hrs_bins, \n",
    "    right=False, \n",
    "    labels=work_week_hrs_labels, \n",
    "    precision=0, \n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Drop missing values\n",
    "work_week_hrs = work_week_hrs.dropna()\n",
    "\n",
    "# Sort the binned data in ascending order\n",
    "work_week_hrs.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# Change the values from categorical to string to be able to plot them\n",
    "work_week_hrs = work_week_hrs.astype(\"str\")\n",
    "\n",
    "# Plot an histogram for the binned work week hours\n",
    "plot_histogram(\n",
    "    work_week_hrs,\n",
    "    work_week_hrs_bins.shape[0], \n",
    "    f\"Work Week Length (Hours)\", \n",
    "    people_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Work Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:04.515195Z",
     "start_time": "2020-02-24T16:11:03.881673Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reword the options stored for work locations\n",
    "work_locs = work_locs.map(\n",
    "    lambda response:\n",
    "        \"NA\" if (response == \"nan\")\n",
    "        else \"Office\" if (response == \"Office\")\n",
    "        else \"Remote\" if (response == \"Home\")\n",
    "        else \"Remote\" if (response == \"Other place, such as a coworking space or cafe\")\n",
    "        else \"Other\"\n",
    ")\n",
    "\n",
    "# Count the frequencies of each work locaation option\n",
    "work_locs_counts = count_options_frequencies(work_locs)\n",
    "\n",
    "# Create a DataFrame for the work locations and their frequencies\n",
    "work_locs_df = prepare_dataframe(\n",
    "    work_locs_counts, \n",
    "    [\"WorkLocation\", \"Respondents\"]\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "work_locs_plot = work_locs_df[work_locs_df[\"WorkLocation\"] != \"NA\"]\n",
    "\n",
    "# Plot a pie chart for the work locations\n",
    "plot_pie_chart(\n",
    "    work_locs_plot, \n",
    "    \"Respondents\", \n",
    "    \"WorkLocation\", \n",
    "    \"Work Location - Office vs Remote \",\n",
    "    people_color_gradient,\n",
    "    show_legend=False,\n",
    "    donut_hole=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Most Important Job Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:05.361073Z",
     "start_time": "2020-02-24T16:11:04.520194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Count the frequencies of each job factor\n",
    "job_factors_counts = count_options_frequencies(job_factors)\n",
    "\n",
    "# Create a DataFrame for the job factors and their frequencies\n",
    "job_factors_df = prepare_dataframe(job_factors_counts, [\"JobFactor\", \"Respondents\"])\n",
    "\n",
    "# Reword the options stored for work locations\n",
    "job_factors_df[\"JobFactor\"] = job_factors_df[\"JobFactor\"].map(\n",
    "    lambda response:\n",
    "        \"NA\" if (response == \"nan\")\n",
    "        else \"Technical compatibility\" if (response == \"Languages, frameworks, and other technologies I'd be working with\")\n",
    "        else \"Culture compatibility\" if (response == \"Office environment or company culture\")\n",
    "        else \"Flexible schedule\" if (response == \"Flex time or a flexible schedule\")\n",
    "        else \"Professional development\" if (response == \"Opportunities for professional development\")\n",
    "        else \"Remote work options\" if (response == \"Remote work options\")\n",
    "        else \"Impact of my work\" if (response == \"How widely used or impactful my work output would be\")\n",
    "        else \"Industry compatibility\" if (response == \"Industry that I'd be working in\")\n",
    "        else \"Company's financial performance\" if (response == \"Financial performance or funding status of the company or organization\")\n",
    "        else \"Department/Team compatibility\" if (response == \"Specific department or team I'd be working on\")\n",
    "        else \"Diversity of the company\" if (response == \"Diversity of the company or organization\")\n",
    "        else \"Other\"\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "job_factors_plot = job_factors_df[job_factors_df[\"JobFactor\"] != \"NA\"]\n",
    "\n",
    "# Plot a column chart for the top 5 job factors\n",
    "plot_column_chart(\n",
    "    job_factors_plot.head(n=5),\n",
    "    \"JobFactor\", \"Respondents\",\n",
    "    \"Respondents\",\n",
    "    \"Five Most Important Job Factors\",\n",
    "    people_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Reason for latest Resume Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:05.958045Z",
     "start_time": "2020-02-24T16:11:05.365075Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reword the options stored for resume update reasons\n",
    "resume_updates = resume_updates.map(\n",
    "    lambda response:\n",
    "        \"NA\" if (response == \"nan\")\n",
    "        else \"New achievement\" if (response == \"Something else changed (education, award, media, etc.)\")\n",
    "        else \"Job search\" if (response == \"I was preparing for a job search\")\n",
    "        else \"Job opportunity\" if (response == \"I heard about a job opportunity (from a recruiter, online job posting, etc.)\")\n",
    "        else \"Job status change\" if (response == \"My job status changed (promotion, new job, etc.)\")\n",
    "        else \"Negative experience at work\" if (response == \"I had a negative experience or interaction at work\")\n",
    "        else \"Re-entry into workforce\" if (response == \"Re-entry into the workforce\")\n",
    "        else \"Other\"\n",
    ")\n",
    "\n",
    "# Count the frequencies of each resume update reason\n",
    "resume_updates_counts = count_options_frequencies(resume_updates)\n",
    "\n",
    "# Create a DataFrame for the resume update reasons and their frequencies\n",
    "resume_updates_df = prepare_dataframe(\n",
    "    resume_updates_counts, \n",
    "    [\"UpdateReason\", \"Respondents\"]\n",
    ")\n",
    "\n",
    "# Filter out blanks for the plot\n",
    "resume_updates_plot = resume_updates_df[\n",
    "    resume_updates_df[\"UpdateReason\"] != \"NA\"\n",
    "]\n",
    "\n",
    "# Plot a column chart for the resume update reasons\n",
    "plot_column_chart(\n",
    "    resume_updates_plot,\n",
    "    \"UpdateReason\", \"Respondents\",\n",
    "    \"Respondents\",\n",
    "    \"Reason for Latest Resume Update\",\n",
    "    people_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Annual Compensation (USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:09.264359Z",
     "start_time": "2020-02-24T16:11:05.961048Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Explore the raw data visually to define its limits\n",
    "# scatter_plot_preview(annual_compensations) # 0-150k USD\n",
    "\n",
    "# Mark the outliers as missing data (less than 0 USD\\\n",
    "# and more than 150k USD)\n",
    "annual_compensations = annual_compensations.where(\n",
    "    annual_compensations >= 0, \n",
    "    np.NaN\n",
    ")\n",
    "annual_compensations = annual_compensations.where(\n",
    "    annual_compensations <= 150000, \n",
    "    np.NaN\n",
    ")\n",
    "\n",
    "# Create the bin labels\n",
    "annual_compensations_labels = pd.Series(\n",
    "    [f\"[{i:,}, {i+25000:,})\" for i in range(0, 125001, 25000)]\n",
    ")\n",
    "\n",
    "# Create the bin intervals\n",
    "annual_compensations_bins = pd.IntervalIndex.from_tuples(\n",
    "    [(i, i+25000) for i in range(0, 125001, 25000)],\n",
    "    closed=\"left\"\n",
    ")\n",
    "\n",
    "# Bin the annual compensation Series into six bins\n",
    "annual_compensations = pd.cut(\n",
    "    annual_compensations, annual_compensations_bins, \n",
    "    right=False, \n",
    "    labels=annual_compensations_labels, \n",
    "    precision=0, \n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Drop missing values\n",
    "annual_compensations = annual_compensations.dropna()\n",
    "\n",
    "# Sort the binned data in ascending order\n",
    "annual_compensations.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# Change the values from categorical to string to be able to plot them\n",
    "annual_compensations = annual_compensations.astype(\"str\")\n",
    "\n",
    "# Plot an histogram for the binned annual compensation\n",
    "plot_histogram(\n",
    "    annual_compensations,\n",
    "    annual_compensations_bins.shape[0], \n",
    "    \"Annual Compensation (USD)\", \n",
    "    people_color,\n",
    "    axes_titles=[None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Almost three quarters of the inquired are developers by profession. Over 10% of the respondents are students and 8% need to code as part of their job;\n",
    "\n",
    "\n",
    "* 44% of the respondents work between 40 to 50 hours per week. 11% work between 30 to 40 hours and 6% between 50 to 60 hours;\n",
    "\n",
    "\n",
    "* In a group of 10 people, close to 6 of them work at an office and 4 work remotely;\n",
    "\n",
    "\n",
    "* Technical compatibility is a deal-breaker for almost half of the inquired when looking for a job. However, culture compatibility, flexible schedule and opportunity for professional development are equally important for about 40% of the respondents;\n",
    "\n",
    "\n",
    "* 37% of the respondents' most recent resume update was to search for a job. A change in job status came in second, reported by 15% of the inquired;\n",
    "\n",
    "\n",
    "* Half of the inquired reported an annual compensation of 25,000\\\\$ or less. In comparison, 13% reported an annual compensation between 25,000\\\\$ and 50,000\\\\$ and 11% between 50,000\\\\$ and 75,000\\\\$. The remaining quarter receive between 75,000\\\\$ and 150,000\\\\$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T21:05:31.243011Z",
     "start_time": "2020-02-22T21:05:31.236029Z"
    },
    "hidden": true
   },
   "source": [
    "After getting to know the respondents through general information, what technologies they use and some aspects of their professional lives, in this last section we will combine data from different sections to answer the last two questions of the analysis:\n",
    "\n",
    "* How does remote work factor in today's professional life (compensation and work week hours);\n",
    "\n",
    "\n",
    "* Tech Stack data vs Compensation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Remote Work in Professional Life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:10.413548Z",
     "start_time": "2020-02-24T16:11:09.266366Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove blanks from the original work location series\n",
    "work_locs = work_locs[work_locs != \"NA\"]\n",
    "\n",
    "# Create a DataFrame using the annual compensations Series\n",
    "compens_workloc_df = pd.DataFrame(annual_compensations)\n",
    "\n",
    "# Join the original work location Series to the DataFrame, using\\\n",
    "# the Series' index. \n",
    "# The inner join assures the DataFrame will have only responses\\\n",
    "# where both the annual compensation and the work location were\\\n",
    "# reported\n",
    "compens_workloc_df = compens_workloc_df.join(work_locs, how=\"inner\")\n",
    "\n",
    "# Plot an histogram for the annual compensation, broken down by\\\n",
    "# work location\n",
    "plot_mult_histogram(\n",
    "    compens_workloc_df,\n",
    "    \"ConvertedComp\",\n",
    "    \"WorkLoc\",\n",
    "    annual_compensations_bins.shape[0],\n",
    "    f\"Annual Compensation vs Work Location\",\n",
    "    axes_titles=[\"Annual Compensation (USD)\", None]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:11.786250Z",
     "start_time": "2020-02-24T16:11:10.418548Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame using the work week hours series\n",
    "workweekhrs_workloc_df = pd.DataFrame(work_week_hrs)\n",
    "\n",
    "# Join the original work location Series to the DataFrame, using\\\n",
    "# the Series' index. \n",
    "# The inner join assures the DataFrame will have only responses\\\n",
    "# where both the work week hours and the work location were\\\n",
    "# reported\n",
    "workweekhrs_workloc_df = workweekhrs_workloc_df.join(\n",
    "    work_locs, \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Plot an histogram of the work week length broken down by work\\\n",
    "# location\n",
    "plot_mult_histogram(\n",
    "    workweekhrs_workloc_df,\n",
    "    \"WorkWeekHrs\",\n",
    "    \"WorkLoc\",\n",
    "    work_week_hrs_bins.shape[0],\n",
    "    f\"Work Week Length (Hours) vs Work Location\",\n",
    "    axes_titles=[\"Work Week Hours\", None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Programming Languages Compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:11:36.023050Z",
     "start_time": "2020-02-24T16:11:11.791257Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove blanks from the original programming language replies\n",
    "languages = languages[languages != \"nan\"]\n",
    "\n",
    "# Since this question was a multiple choice question, each\\\n",
    "# reply can have more than one option, separated by semicolons.\\\n",
    "# Thus, the individual options need to be separated into different\\\n",
    "# rows, resulting in a multi-level index: the first level represents\\\n",
    "# each reply and the second level each option included in that reply\n",
    "languages_unpivot = unpivot_delimited_data(languages, \";\")\n",
    "\n",
    "# Reset the compensations' index so that we have a column\\\n",
    "# with the index to select during the merge \n",
    "annual_compensations_reset = annual_compensations.reset_index()\n",
    "\n",
    "# Merge the unpivoted data with the annual compensations, leaving\\\n",
    "# out replies where one of the questions was not answered\n",
    "languages_compens_df = merge_unpivoted_data(\n",
    "    languages_unpivot, \n",
    "    annual_compensations_reset, \n",
    "    \"index\", \n",
    "    \"Language\", \n",
    "    \"ConvertedComp\"\n",
    ")\n",
    "\n",
    "# Get the top 10 programming languages, in descending order\n",
    "top_10_langs = languages_plot.head(n=10)[\"Language\"].unique()\n",
    "\n",
    "# Remove the rows with languages not in the top 10\n",
    "languages_compens_df = languages_compens_df[\n",
    "    languages_compens_df[\"Language\"].isin(top_10_langs)\n",
    "]\n",
    "\n",
    "# Convert the column of unpivoted data into the Categorical\\\n",
    "# data type, using the top n list as the available categories.\\\n",
    "# This makes it so that when the data is sorted it keeps the\\\n",
    "# order of the top n list\n",
    "languages_compens_df = sort_by_top(\n",
    "    languages_compens_df, \n",
    "    \"Language\", \n",
    "    top_10_langs\n",
    ")\n",
    "\n",
    "# Group the DataFrame by programming language and find the\\\n",
    "# most common compensation interval for each\n",
    "languages_compens_mode = get_mode_df(\n",
    "    languages_compens_df, \n",
    "    \"Language\", \n",
    "    \"ConvertedComp\"\n",
    ")\n",
    "\n",
    "# Show a table with the most common compensation interval for\\\n",
    "# each programming language\n",
    "plot_table(\n",
    "    languages_compens_mode, \n",
    "    [\"Programming Language\", \"Compensation Interval\"], \n",
    "    [\"Language\", \"ConvertedComp\"],\n",
    "    \"Most common Annual Compensation level (USD)<br>for the top 10 Programming Languages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Database Environments Compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:12:02.351124Z",
     "start_time": "2020-02-24T16:11:36.028061Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove blanks from the original database replies\n",
    "databases = databases[databases != \"nan\"]\n",
    "\n",
    "# Since this question was a multiple choice question, each\\\n",
    "# reply can have more than one option, separated by semicolons.\\\n",
    "# Thus, the individual options need to be separated into different\\\n",
    "# rows, resulting in a multi-level index: the first level represents\\\n",
    "# each reply and the second level each option included in that reply\n",
    "databases_unpivot = unpivot_delimited_data(databases, \";\")\n",
    "\n",
    "# Merge the unpivoted data with the annual compensations, leaving\\\n",
    "# out replies where one of the questions was not answered\n",
    "databases_compens_df = merge_unpivoted_data(\n",
    "    databases_unpivot, \n",
    "    annual_compensations_reset, \n",
    "    \"index\", \n",
    "    \"Database\", \n",
    "    \"ConvertedComp\"\n",
    ")\n",
    "\n",
    "# Get the top 10 databases, in descending order\n",
    "top_10_dbs = databases_plot.head(n=10)[\"Database\"].unique()\n",
    "\n",
    "# Remove the rows with databases not in the top 10\n",
    "databases_compens_df = databases_compens_df[\n",
    "    databases_compens_df[\"Database\"].isin(top_10_dbs)\n",
    "]\n",
    "\n",
    "# Convert the column of unpivoted data into the Categorical\\\n",
    "# data type, using the top n list as the available categories.\\\n",
    "# This makes it so that when the data is sorted it keeps the\\\n",
    "# order of the top n list\n",
    "databases_compens_df = sort_by_top(\n",
    "    databases_compens_df, \n",
    "    \"Database\", \n",
    "    top_10_dbs\n",
    ")\n",
    "\n",
    "# Group the DataFrame by database and find the most common\\\n",
    "# compensation interval for each\n",
    "databases_compens_mode = get_mode_df(\n",
    "    databases_compens_df, \n",
    "    \"Database\", \n",
    "    \"ConvertedComp\"\n",
    ")\n",
    "\n",
    "# Show a table with the most common compensation interval for\\\n",
    "# each database environment\n",
    "plot_table(\n",
    "    databases_compens_mode, \n",
    "    [\"Database Environment\", \"Compensation Interval\"], \n",
    "    [\"Database\", \"ConvertedComp\"],\n",
    "    \"Most common Annual Compensation level (USD)<br>for the top 10 Database Environments\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Web Development Compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:12:23.607010Z",
     "start_time": "2020-02-24T16:12:02.353115Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove blanks from the original database replies\n",
    "web_frameworks = web_frameworks[web_frameworks != \"nan\"]\n",
    "\n",
    "# Since this question was a multiple choice question, each\\\n",
    "# reply can have more than one option, separated by semicolons.\\\n",
    "# Thus, the individual options need to be separated into different\\\n",
    "# rows, resulting in a multi-level index: the first level represents\\\n",
    "# each reply and the second level each option included in that reply\n",
    "web_frameworks_unpivot = unpivot_delimited_data(web_frameworks, \";\")\n",
    "\n",
    "# Merge the unpivoted data with the annual compensations, leaving\\\n",
    "# out replies where one of the questions was not answered\n",
    "web_frameworks_compens_df = merge_unpivoted_data(\n",
    "    web_frameworks_unpivot, \n",
    "    annual_compensations_reset, \n",
    "    \"index\", \n",
    "    \"WebFramework\", \n",
    "    \"ConvertedComp\"\n",
    ")\n",
    "\n",
    "# Get the top 10 web frameworks, in descending order\n",
    "top_10_webfws = web_frameworks_plot.head(n=10)[\"WebFramework\"].unique()\n",
    "\n",
    "# Remove the rows with web frameworks not in the top 10\n",
    "web_frameworks_compens_df = web_frameworks_compens_df[\n",
    "    web_frameworks_compens_df[\"WebFramework\"].isin(top_10_webfws)\n",
    "]\n",
    "\n",
    "# Convert the column of unpivoted data into the Categorical\\\n",
    "# data type, using the top n list as the available categories.\\\n",
    "# This makes it so that when the data is sorted it keeps the\\\n",
    "# order of the top n list\n",
    "web_frameworks_compens_df = sort_by_top(\n",
    "    web_frameworks_compens_df, \n",
    "    \"WebFramework\", \n",
    "    top_10_webfws\n",
    ")\n",
    "\n",
    "# Group the DataFrame by web frameworks and find the most common\\\n",
    "# compensation interval for each\n",
    "# web_frameworks_compens_mode = web_frameworks_compens_df.groupby([\"WebFramework\"])\\\n",
    "#     .apply( lambda x: x[\"ConvertedComp\"].mode() )\n",
    "web_frameworks_compens_mode = get_mode_df(\n",
    "    web_frameworks_compens_df, \n",
    "    \"WebFramework\", \n",
    "    \"ConvertedComp\"\n",
    ")\n",
    "\n",
    "# Show a table with the most common compensation interval for\\\n",
    "# each web framework\n",
    "plot_table(\n",
    "    web_frameworks_compens_mode, \n",
    "    [\"Web Framework\", \"Compensation Interval\"], \n",
    "    [\"WebFramework\", \"ConvertedComp\"],\n",
    "    \"Most common Annual Compensation level (USD)<br>for the top 10 Web Development Frameworks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Looking at the respondents who reported both their annual compensation and their work location, working in person at an office has a small advantage only for those with an annual compensation below 75,000$;\n",
    "\n",
    "\n",
    "* Looking at those who reported both their work week hours and their work location, roughly two out of three respondents that work between 30 to 60 hours per week reported working in an office;\n",
    "\n",
    "\n",
    "* As expected, given the data seen previously for annual compensation, the most common level of annual compensation for any programming language, database environment or web development framework is less than 25,000$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### General Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Over 40% of the respondents were between the ages of 20 and 30 and almost 70% were between 20 and 40 years-old;\n",
    "\n",
    "\n",
    "* Over 90% of the inquired were male, with less than 8% being of the female gender. The remaining 2% identified as another gender;\n",
    "\n",
    "\n",
    "* About 43% of the respondents completed a Bachelor's Degree, but only half of those have gone on to complete a Master's Degree;\n",
    "\n",
    "\n",
    "* More than half of respondents come from a Computer Science or Software Engineering academic background. The remaining inquired come primarily from other Engineering disciplines, Information Technologies, System Administration or Web Design and/or Development;\n",
    "\n",
    "\n",
    "* 70% of the respondents have a full-time job, with freelance and/or self-employment being the second most common employment status;\n",
    "\n",
    "\n",
    "* The five countries with most respondents are, from most to least, the United States of America, India, Germany, the United Kingdom and Canada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Tech Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Over 60% of the respondents use JavaScript and/or HTML/CSS, over 50% use SQL and 40% use Python and/or Java;\n",
    "\n",
    "\n",
    "* Over 46% of the respondents use MySQL for database environments. On the other hand, only about 23% use PostgreSQL and/or Microsoft SQL Server, the second and third most popular environments, respectively;\n",
    "\n",
    "\n",
    "* jQuery is still the most used Web Framework, used by almost a third of the inquired. React.js comes in second place (22%) and Angular(.js) in a very close third place;\n",
    "\n",
    "\n",
    "* In a group of 10 developers, about 5 of them reported using Visual Studio Code as one of their IDEs/Development Environments. About 3 people in the group use Visual Studio and/or Notepad++ as well;\n",
    "\n",
    "\n",
    "* Windows is the operating system of choice of almost half of the respondents. The remaining half is fairly evenly divided between MacOS and Linux-based systems, but the former takes second place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Professional Life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Almost three quarters of the inquired are developers by profession. Over 10% of the respondents are students and 8% need to code as part of their job;\n",
    "\n",
    "\n",
    "* 44% of the respondents work between 40 to 50 hours per week. 11% work between 30 to 40 hours and 6% between 50 to 60 hours;\n",
    "\n",
    "\n",
    "* In a group of 10 people, close to 6 of them work at an office and 4 work remotely;\n",
    "\n",
    "\n",
    "* Technical compatibility is a deal-breaker for almost half of the inquired when looking for a job. However, culture compatibility, flexible schedule and opportunity for professional development are equally important for about 40% of the respondents;\n",
    "\n",
    "\n",
    "* 37% of the respondents' most recent resume update was to search for a job. A change in job status came in second, reported by 15% of the inquired;\n",
    "\n",
    "\n",
    "* Half of the inquired reported an annual compensation of 25,000\\\\$ or less. In comparison, 13% reported an annual compensation between 25,000\\\\$ and 50,000\\\\$ and 11% between 50,000\\\\$ and 75,000\\\\$. The remaining quarter receive between 75,000\\\\$ and 150,000\\\\$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Looking at the respondents who reported both their annual compensation and their work location, working in person at an office has a small advantage only for those with an annual compensation below 75,000$;\n",
    "\n",
    "\n",
    "* Looking at those who reported both their work week hours and their work location, roughly two out of three respondents that work between 30 to 60 hours per week reported working in an office;\n",
    "\n",
    "\n",
    "* As expected, given the data seen previously for annual compensation, the most common level of annual compensation for any programming language, database environment or web development framework is less than 25,000$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
